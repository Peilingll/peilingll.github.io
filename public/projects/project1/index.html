<!DOCTYPE html>
<html lang="en">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PEILING SONG</title>
  <link rel="icon" href="/images/favicon.png" type="image/png" />
  
  

  
  
    
  
  
    
      <link rel="stylesheet" href="/css/main.css">
    
  

  

  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

  
  
  
  <link rel="stylesheet" href="/css/hero-base.min.css">
  
  
  <link rel="stylesheet" href="/css/hero-typography.min.css">
  
  
  <link rel="stylesheet" href="/css/hero-animations.min.css">
  
  
  
  <link rel="stylesheet" href="/css/cursor.min.css">
  
  
</head>
<body>
  <div id="custom-cursor"></div>
  <header class="sticky top-0 z-50 bg-white/50 backdrop-blur border-b border-gray-200">
  <div class="max-w-6xl mx-auto flex justify-between items-center px-6 py-4">
    <a href="/" class="text-2xl font-bold">PEILING</a>
    
    
    <button class="hamburger-menu" id="hamburger-menu" aria-label="Toggle menu">
      <span></span>
      <span></span>
      <span></span>
    </button>
    
    
    <nav class="space-x-6 text-sm font-medium" id="nav-menu">
      <a href="/#about" class="hover:underline transition-colors duration-200" 
         style="color: inherit;" 
         onmouseover="this.style.color='#7e6afc'" 
         onmouseout="this.style.color='inherit'">About</a>
      <a href="/#experience" class="hover:underline transition-colors duration-200" 
         style="color: inherit;" 
         onmouseover="this.style.color='#7e6afc'" 
         onmouseout="this.style.color='inherit'">Experience</a>
      <a href="/projects/" class="hover:underline transition-colors duration-200" 
         style="color: inherit;" 
         onmouseover="this.style.color='#7e6afc'" 
         onmouseout="this.style.color='inherit'">Work & Research</a>
      <a href="/blog/" class="hover:underline transition-colors duration-200" 
         style="color: inherit;" 
         onmouseover="this.style.color='#7e6afc'" 
         onmouseout="this.style.color='inherit'">Blog</a>
      <a href="/tags/" class="hover:underline transition-colors duration-200" 
         style="color: inherit;" 
         onmouseover="this.style.color='#7e6afc'" 
         onmouseout="this.style.color='inherit'">Tags</a>
    </nav>
  </div>
</header>

  <main>
    
<main class="max-w-4xl mx-auto px-6 py-12">
  
  
  <header class="mb-10">
    <h1 class="text-4xl font-bold mb-2">Real-Time Building Digital Model Reconstruction Using a Robotic Agent</h1>
    
    
      <h2 class="text-xl text-gray-700 mb-2">Software Lab</h2>
    
    
    <p class="text-sm text-gray-500 mb-4">
      July 18, 2025
    </p>

    
      <p class="text-sm text-gray-700"><strong>Unit:</strong> Chair of Computational Modeling and Simulation TUM Department of Civil, Geo and Environmental Engineering Technical University of Munich</p>
    

    
      <p class="text-sm text-gray-700"><strong>Supervised by:</strong> Prof. Dr.-Ing. André Borrmann/ Mohammad Reza Kolani/ Mansour Mehranfar/ CMS </p>
    

    
      <p class="text-sm text-gray-700"><strong>Contributors:</strong> Pei-Ling Song, Anisha Sinha, Anoushk Kolagotla, Kit Lung Chan</p>
    

    
    
    <div class="mt-4 flex flex-wrap gap-2">
      
        <a href="/tags/project" 
           class="text-decoration font-medium"
           style="color: #7e6afc; text-decoration: none;"
           onmouseover="this.style.color='#FACC15'; this.style.textDecoration='underline';"
           onmouseout="this.style.color='#7e6afc'; this.style.textDecoration='none';">
           #Project
        </a>
      
        <a href="/tags/point-cloud" 
           class="text-decoration font-medium"
           style="color: #7e6afc; text-decoration: none;"
           onmouseover="this.style.color='#FACC15'; this.style.textDecoration='underline';"
           onmouseout="this.style.color='#7e6afc'; this.style.textDecoration='none';">
           #Point Cloud
        </a>
      
        <a href="/tags/simulation" 
           class="text-decoration font-medium"
           style="color: #7e6afc; text-decoration: none;"
           onmouseover="this.style.color='#FACC15'; this.style.textDecoration='underline';"
           onmouseout="this.style.color='#7e6afc'; this.style.textDecoration='none';">
           #Simulation
        </a>
      
        <a href="/tags/semantic" 
           class="text-decoration font-medium"
           style="color: #7e6afc; text-decoration: none;"
           onmouseover="this.style.color='#FACC15'; this.style.textDecoration='underline';"
           onmouseout="this.style.color='#7e6afc'; this.style.textDecoration='none';">
           #Semantic
        </a>
      
        <a href="/tags/robotics" 
           class="text-decoration font-medium"
           style="color: #7e6afc; text-decoration: none;"
           onmouseover="this.style.color='#FACC15'; this.style.textDecoration='underline';"
           onmouseout="this.style.color='#7e6afc'; this.style.textDecoration='none';">
           #Robotics
        </a>
      
        <a href="/tags/munich-germany" 
           class="text-decoration font-medium"
           style="color: #7e6afc; text-decoration: none;"
           onmouseover="this.style.color='#FACC15'; this.style.textDecoration='underline';"
           onmouseout="this.style.color='#7e6afc'; this.style.textDecoration='none';">
           #Munich, Germany
        </a>
      
    </div>
    
  </header>

    
    <article class="prose prose-neutral max-w-none prose-h2:text-2xl prose-h2:font-bold prose-h2:mt-8 prose-h2:mb-4 lightbox">
      <h2 id="introduction">Introduction</h2>
<p>The project is designed to capture and process real-time spatial data of the built environment by integrating advanced sensing technologies, such as LiDAR and an RGB-D camera, onto a robotic platform. These sensors work in unison to collect high-quality spatial and visual data, which is then seamlessly streamed using the Robot Operating System (ROS). ROS facilitates the efficient handling of sensor data, enabling real-time processing through advanced algorithms that reconstruct the environment&rsquo;s geometry with precision. By combining robotics, state-of-the-art sensors, advanced point cloud processing techniques, and real-time computation, the project aims to provide accurate and dynamic spatial mapping solutions, offering a robust foundation for appli￾cations in fields such as architecture, urban planning, and autonomous navigation.</p>
<p><img src="/images/pj1.gif" alt="Dashboard">
<em>Figure 1. Go2 System Trial conducted in the corridors outside Room 4171 at TUM campus</em></p>
<h2 id="methodology">Methodology</h2>
<ol>
<li>
<p><strong>Device Setup</strong><br>
The Unitree Go2 robot is equipped with a LiDAR sensor and an onboard mini-PC, enabling autonomous navigation and real-time data acquisition.</p>
</li>
<li>
<p><strong>Real-Time Data Processing</strong></p>
<ul>
<li><strong>Data Collection</strong>: Capturing raw point cloud data of the built environment.</li>
<li><strong>Preprocessing</strong>: Applying semantic segmentation to classify and refine spatial elements.</li>
<li><strong>Transformation</strong>: Converting segmented data into structured geometric models.</li>
</ul>
</li>
<li>
<p><strong>Export</strong><br>
The reconstructed model is exported in IFC format, ensuring interoperability with BIM workflows and downstream applications in architecture, engineering, and construction.</p>
</li>
</ol>
<p><img src="/images/pj1-1.webp" alt="Dashboard">
<em>Figure 2. Methodology</em></p>
<h2 id="device-setup">Device Setup</h2>
<div class="not-prose" 
     style="display: flex; gap: 1rem; margin: 1rem auto; width: 80%; justify-content: center;">
  <div style="flex: 1; text-align: center;">
    <img src="/images/pj1-2.webp" alt="Dashboard" style="width:100%; border-radius: 8px;" />
    <p style="color: gray; font-style: italic; font-size: 0.875rem; margin-top: 0.5rem; text-align: center;">
      Figure 3. Connection Diagram – Go2 SLAM System
  </div>
  <div style="flex: 1; text-align: center;">
    <img src="/images/pj1-3.webp" alt="Dashboard" style="width:100%; border-radius: 8px;" />
    <p style="color: gray; font-style: italic; font-size: 0.875rem; margin-top: 0.5rem; text-align: center;">
      Figure 4. ROS2-Rviz Viewe
    </p>
  </div>
</div>
<h2 id="data-collection">Data Collection</h2>
<p><img src="/images/pj1-4.webp" alt="Dashboard">
<em>Figure 5. Point Cloud from Field Survey Trial</em></p>
<h2 id="data-processing">Data Processing </h2>
<h3 id="semantic-segmentation-framework">Semantic Segmentation Framework</h3>
<ol>
<li>
<p><strong>View Generation</strong><br>
The raw point cloud is randomly cropped and masked to produce global, local, and masked views, allowing the model to learn from incomplete spatial perspectives.</p>
</li>
<li>
<p><strong>Dual Encoder Framework</strong></p>
<p>Two encoders based on <strong>Point Transformer V3 (PTv3)</strong> are trained in parallel, with weights synchronized through <strong>Exponential Moving Average (EMA)</strong> to ensure stable convergence.</p>
</li>
<li>
<p><strong>Self-Distillation</strong><br>
Reference and query encoders align their features using <strong>Sinkhorn-Knopp Centering</strong>, producing consistent embeddings for robust representation learning.</p>
</li>
</ol>
<p>The resulting learned features provide a strong foundation for accurate and efficient semantic segmentation of complex 3D environments.
<img src="/images/pj1-5.webp" alt="Dashboard">
<em>Figure 6. Semantic segmentation preprocessing</em></p>
<h3 id="point-cloud-classification">Point Cloud Classification</h3>
<p>We implement a <strong>semantic segmentation pipeline</strong> to classify point cloud data of the built environment. The process begins with subsampling the raw point cloud and estimating surface <strong>normals using KNN</strong>. <strong>Floor and ceiling surfaces</strong> are then automatically extracted by analyzing normal orientations and clustering points with <strong>DBSCAN</strong> to form large planar regions. The <strong>remaining points</strong> are classified using a deep learning model trained on the <strong>ScanNet-20 dataset</strong>, predicting semantic categories such as <strong>walls, furniture, and doors</strong>.  Finally, the results are merged and exported as a <strong>classified PLY file</strong>, providing a structured and interpretable 3D representation.</p>
<p><img src="/images/pj1-6.webp" alt="Dashboard">
<em>Figure 7. Semantic segmentation preprocessing</em></p>
<h3 id="geometric-modeling">Geometric Modeling</h3>
<p>In this step, we generate a simplified 3D model from point cloud data. For floors, we use RANSAC to reduce noise and focus on the main surface. For walls—we apply DBSCAN clustering to g separate disconnected wall segments and use interquartile range (IQR) filtering to remove Z-axis outliers. Next, split the XY plane into grids. and an axis-aligned bounding box is generated for each cell. Then, these bounding boxes are used to build the 3D geometry model and export it as an OBJ file. In the next phase, we plan to convert it to an IFC</p>
<p><img src="/images/pj1-7.webp" alt="Dashboard">
<em>Figure 8. Geometric Modeling</em></p>
<h2 id="future-work">Future Work</h2>
<ul>
<li>
<p><strong>Integrate door and window classification</strong><br>
Detect and label fixtures for richer BIM outputs.</p>
</li>
<li>
<p><strong>Full Geometry-to-IFC Pipeline</strong><br>
Convert extracted primitives into <strong>IfcWall</strong>, <strong>IfcSlab</strong>, and <strong>IfcFurnishingElements</strong> automatically.</p>
</li>
<li>
<p><strong>Real-Time Process Orchestration</strong><br>
Stream SLAM &gt; Segmentation &gt; Bounding-box fitting &gt; IFC export in a single workflow.</p>
</li>
<li>
<p><strong>Hardware Upgrade: Camera Fusion</strong><br>
Combine LiDAR with RGB/depth cameras for improved coverage and texture capture.</p>
</li>
</ul>
<h2 id="references">References</h2>
<hr>
<div style="margin-bottom:0.8em; padding-left:2em; text-indent:-2em;">
[1] Dong, S., Xu, K., Zhou, Q., Tagliasacchi, A., Xin, S., Nießner, M., & Chen, B. (2019). Multi-robot collaborative dense scene reconstruction. <i>ACM Transactions on Graphics (TOG), 38</i>(4), 1–16. https://doi.org/10.1145/3306346.3322997
</div>
<div style="margin-bottom:0.8em; padding-left:2em; text-indent:-2em;">
[2] Xu, K., Zheng, L., Yan, Z., Yan, G., Zhang, E., Nießner, M., Deussen, O., Cohen-Or, D., & Huang, H. (2017). Autonomous reconstruction of unknown indoor scenes guided by time-varying tensor fields. <i>ACM Transactions on Graphics (TOG), 36</i>(6), 1–14. https://doi.org/10.1145/3130800.3130830
</div>
<div style="margin-bottom:0.8em; padding-left:2em; text-indent:-2em;">
[3] Liu, L., Xia, X., Sun, H., Shen, Q., Xu, J., Chen, B., Huang, H., & Xu, K. (2018). Object-aware guidance for autonomous scene reconstruction. <i>ACM Transactions on Graphics (TOG), 37</i>(4), 1–11. https://doi.org/10.1145/3197517.3201376
</div>
<div style="margin-bottom:0.8em; padding-left:2em; text-indent:-2em;">
[4] Mehranfar, M., Braun, A., & Borrmann, A. (2024). From dense point clouds to semantic digital models: End-to-end AI-based automation procedure for Manhattan-world structures. <i>Automation in Construction, 162,</i> 105392. https://doi.org/10.1016/j.autcon.2024.105392
</div>
<div style="margin-bottom:0.8em; padding-left:2em; text-indent:-2em;">
[5] Franz, S., Irmler, R., & Rüppel, U. (2018). Real-time collaborative reconstruction of digital building models with mobile devices. <i>Advanced Engineering Informatics, 38,</i> 569–580. https://doi.org/10.1016/j.aei.2018.07.004
</div>
<div style="margin-bottom:0.8em; padding-left:2em; text-indent:-2em;">
[6] Gao, X., Zhang, S., Liu, Y., et al. (2024). A new framework for generating indoor 3D digital models from point clouds. <i>Remote Sensing, 16</i>(18), 3462. https://doi.org/10.3390/rs16183462
</div>
<div style="margin-bottom:0.8em; padding-left:2em; text-indent:-2em;">
[7] Segal, A., Haehnel, D., & Thrun, S. (2009). Generalized-ICP. In <i>Robotics: Science and Systems</i> (Vol. 2, No. 4). https://doi.org/10.15607/RSS.2009.V.021
</div>
<div style="margin-bottom:0.8em; padding-left:2em; text-indent:-2em;">
[8] Ali, M. K., Hussain, A., Chen, X., et al. (2019). Multi-sensor depth fusion framework for real-time 3D reconstruction. <i>IEEE Access, 7,</i> 136471–136480. https://doi.org/10.1109/ACCESS.2019.2942145
</div>
<div style="margin-bottom:0.8em; padding-left:2em; text-indent:-2em;">
[9] Chair of Computational Modeling and Simulation. (n.d.). Mobile machinery lab. Technical University of Munich (TUM). https://www.cms.bgu.tum.de
</div>
<div style="margin-bottom:0.8em; padding-left:2em; text-indent:-2em;">
[10] Pan, Y. (2023). Creating an information-rich digital twin of indoor environments by interpretation and fusion of image and point-cloud data (Doctoral dissertation, Technische Universität München). https://mediatum.ub.tum.de
</div>
<div style="margin-bottom:0.8em; padding-left:2em; text-indent:-2em;">
[11] Mohanarajah, G., Usenko, V., Singh, M., D’Andrea, R., & Waibel, M. (2015). Cloud-based collaborative 3D mapping in real time with low-cost robots. <i>IEEE Transactions on Automation Science and Engineering, 12</i>(2), 423–431. https://doi.org/10.1109/TASE.2014.2368991
</div>

    </article>
    
    
    <div id="lightbox-overlay" class="fixed inset-0 bg-black bg-opacity-75 hidden items-center justify-center z-50">
      <img id="lightbox-img" src="" alt="Expanded image" class="max-h-[90%] max-w-[90%] rounded-lg shadow-lg" />
    </div>

    <script>
      document.addEventListener("DOMContentLoaded", function () {
        const images = document.querySelectorAll(".lightbox img");
        const overlay = document.getElementById("lightbox-overlay");
        const overlayImg = document.getElementById("lightbox-img");
    
        images.forEach(img => {
          img.addEventListener("click", () => {
            overlayImg.src = img.src;
            overlay.classList.add("active");
          });
        });
    
        overlay.addEventListener("click", () => {
          overlay.classList.remove("active");
          overlayImg.src = "";
        });
      });
    </script>
    
    
  

</main>

  </main>

  <footer class="text-center py-6 mt-10 border-t">
  <p class="text-sm text-gray-500">Munich, Germany | © 2025 PEI LING SONG</p>
  <div class="flex justify-center gap-4">
    <a href="https://www.linkedin.com/in/peiling-song" target="_blank" class="text-gray-600 hover:text-[#7e6afc] transition-colors">
      <i class="fab fa-linkedin text-1xl"></i>
    </a>
    <a href="mailto:peiling.song@tum.de" class="text-gray-600 hover:text-[#7e6afc] transition-colors">
      <i class="fas fa-envelope text-1xl"></i>
    </a>
    <a href="https://github.com/your-github" target="_blank" class="text-gray-600 hover:text-[#7e6afc] transition-colors">
      <i class="fab fa-github text-1xl"></i>
    </a>
  </div>
</footer>

  
  
  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.12.2/gsap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.12.2/CustomEase.min.js"></script>
  
  
  
  <script src="/js/hero-main.min.js"></script>
  
  
  <script src="/js/text-effects.min.js"></script>
  
  
  <script src="/js/background-text.min.js"></script>
  
  
  
  <script src="/js/cursor.min.js"></script>
  
  
  <script src="/js/hero-balls.min.js" defer></script>

  
  
  <script src="/js/hamburger-menu.min.js"></script>

  
</body>
</html>